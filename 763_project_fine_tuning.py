# -*- coding: utf-8 -*-
"""763_Project_Fine_Tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i5bsRGmt-zzjipiE040gQ4A9aSYyX_lg

# SEIS-763 Group Project include group member names with an introduction explaining what problem we are solving (NEEDS REVISION)
- We are a state program that provides low interest loans to low income family.
- The maximum amount that we are allowed to approve on a loan up to $300,000
- Default rates tend to be high and we have a limited budget, therefore our goal is to maximize the amount allocated to our agency by predicting home values based on a multitude of different predictors in order to best mitigate loss if a home goes into default and maximize purchasing power to help as many families as possible.
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Step 1. Data Cleaning/Imputation


"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.metrics import classification_report
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import OrdinalEncoder

# Load data
file_path_train = '/content/drive/MyDrive/train.csv'
file_path_test = '/content/drive/MyDrive/test.csv'
df1 = pd.read_csv(file_path_train)
df2 = pd.read_csv(file_path_test)

# Combine (stack) them vertically, one after the other
df = pd.concat([df1, df2], ignore_index=True)

num_rows = df.shape
print(f"Number of rows in the dataset: {num_rows}")
print(df.head())

# Drop duplicates
df = df.drop_duplicates()
print("After dropping duplicates: The number of rows: ", df.shape)

# Show initial missing data
total_missing = df.isnull().sum().sum()
print(f"Total missing values before imputation: {total_missing}")

"""### Drop any rows that are missing a 'SalePrice' value prior to classification (David-Start)"""

df = df.dropna(subset=['SalePrice'])
print(df.shape)

# Separate numeric & categorical
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
cat_cols = df.select_dtypes(include=['object']).columns

# Ordinal encode categorical data (KNN needs numeric input)
encoder = OrdinalEncoder()
encoded_cats = encoder.fit_transform(df[cat_cols])

# Combine numeric and encoded categorical
combined = np.hstack((df[numeric_cols], encoded_cats))

# Plot a histogram of SalePrice
plt.figure(figsize=(10, 5))
plt.hist(df['SalePrice'], bins=50, edgecolor='black')
plt.title('Histogram of SalePrice')
plt.xlabel('SalePrice')
plt.ylabel('Number of Homes')
plt.grid(axis='y', alpha=0.5)
plt.show()

# Plot a boxplot of SalePrice
plt.figure(figsize=(10, 2))
plt.boxplot(df['SalePrice'], vert=False, patch_artist=True)
plt.title('Boxplot of SalePrice')
plt.xlabel('SalePrice')
plt.show()

"""### Address Missing Values
- Addpy KNN Imputer
"""

# Apply KNN imputer
imputer = KNNImputer(n_neighbors=5)
imputed = imputer.fit_transform(combined)

# Reconstruct dataframe
imputed_numeric = pd.DataFrame(imputed[:, :len(numeric_cols)], columns=numeric_cols)
imputed_cats = pd.DataFrame(imputed[:, len(numeric_cols):], columns=cat_cols)

# Decode categorical back to original labels
decoded_cats = pd.DataFrame(encoder.inverse_transform(imputed_cats), columns=cat_cols)

# Combine numeric and categorical
df_imputed = pd.concat([imputed_numeric, decoded_cats], axis=1)

# Verify
print(f"Total missing values after imputation: {df_imputed.isnull().sum().sum()}")
print("KNN numeric + categorical imputation completed.")

"""### Create a Histogram and a Boxplot of the SalePrice using the dataset to determine the price range of our classes.

# Step 4. Remove outliers (IQR, Z-score, etc.) + Step 6. Apply Classification to the target

### Remove Outliers on our Target (SalePrice) Based on Low Interest Government Financing to Low Income Families
"""

outliers = {}
q1 = df_imputed['SalePrice'].quantile(0.25)
q3 = df_imputed['SalePrice'].quantile(0.75)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr

# Set upper limit to $300,000 due to Organizational Business Rules
mortgage_upper = np.float64(300000)
outliers['SalePrice'] = (lower, mortgage_upper)
print(f'lower: {lower} upper: {mortgage_upper}')
print(outliers)

"""## Use Classification on the SalePrice column (target) over predicting SalePrice Directly we have determined provides the following:

### Risk Management:
- Price bins clearly correlate to risk and affordability, helping loan officers make better lending decisions.
- Predicting a range is easier and more actionable than pinpointing exact home price, especially given limited data (1,500 records).

### Interpretability:
- Price categories are easily explained to non-technical stakeholders (loan officers, regulators, buyers).

### Robustness and Stability
- Predicting precise numeric prices is harder due to high variability and data noise.
- Classification stabilizes predictions into clearly actionable bins.
"""

min_price = lower
max_price = mortgage_upper   # Or use df['SalePrice'].max() for the max in your data
step = 15000

# Step 1: Create meaningful, fixed bins (apply classification on the SalePrice every 15k between lower and upper range or our trimmed dataset)
min_price = 0
max_price = 300000   # Or use df['SalePrice'].max() for the max in your data
step = 15000

# Create bin edges
bins = np.arange(min_price, max_price + step, step)  # e.g., 0, 15k, 30k, ..., 300k

# Create labels for each bin
labels = [f'${bins[i]:,.0f}â€“${bins[i+1]:,.0f}' for i in range(len(bins)-1)]

df_imputed['PriceCategory'] = pd.cut(df['SalePrice'], bins=bins, labels=labels, include_lowest=True)

# Display the counts per category after filtering
print(df_imputed['PriceCategory'].value_counts().sort_index())

import pandas as pd
import matplotlib.pyplot as plt
bin_counts = df_imputed['PriceCategory'].value_counts().sort_index()

plt.figure(figsize=(8, 5))
bin_counts.plot(kind='barh')  # <-- horizontal bars
plt.ylabel('Price Bin')       # Bin labels on Y axis
plt.xlabel('Number of Records')
plt.title('Number of Records per Price Bin')
plt.tight_layout()
plt.show()

# Identify categorical columns to encode (excluding PriceCategory)
categorical_cols_to_encode = df_imputed.select_dtypes(include=['object', 'category']).columns.tolist()
if 'PriceCategory' in categorical_cols_to_encode:
    categorical_cols_to_encode.remove('PriceCategory')

# Apply one-hot encoding to the selected categorical columns
df_imputed = pd.get_dummies(df_imputed, columns=categorical_cols_to_encode, prefix='PC')

# Quickly check the distribution of the target variable BEFORE one-hot encoding it
print(df_imputed['PriceCategory'].value_counts().sort_index())

# Create a copy of the DataFrame to work with for filtering and subsequent steps.
# df_imputed is already filtered for NaN PriceCategory in cell Qvu-cklN_oC-
df_filtered = df_imputed.copy()

# Ensure PriceCategory is present for subsequent steps, assuming it was created earlier
if 'PriceCategory' not in df_filtered.columns:
    print("Warning: 'PriceCategory' column not found. Please ensure it was created in a previous step.")

# Check if the dummy columns for PriceCategory exist and revert if necessary
pc_dummy_cols = [col for col in df_filtered.columns if col.startswith('PC_')]
if pc_dummy_cols:
    # A better approach is to fix the one-hot encoding in HY8rdCVXVYZV directly.
    print("Attempting to revert PriceCategory one-hot encoding if it occurred.")

    # I will rely on fixing HY8rdCVXVYZV to not one-hot encode PriceCategory.
    # For now, just ensure df_filtered is a copy and the problematic filtering is removed.
    pass # The fix is primarily removing the filtering line.


print(df_filtered.shape)

print(df_imputed['PriceCategory'].value_counts().sort_index())

"""### Classifying the SalePrice Colummn: Complete

## Continue cleaning the data:
- Standardize the Data
- One-hot-Encode the Categorical Predictors

### One-hot encode other categorical variables, but NOT PriceCategory as it's the target
"""

numerical_columns = df_filtered.select_dtypes(include=['int64', 'float64']).columns.tolist()
print(numerical_columns)

import scipy.stats as stats

df_filtered[numerical_columns] = df_filtered[numerical_columns].apply(stats.zscore)

other_columns = df_filtered.select_dtypes(include=['object']).columns.tolist()
print(other_columns)

df_filtered = pd.get_dummies(df_filtered, columns=other_columns, drop_first=True)
print(df_filtered.shape)
print(df_filtered.head())

#print(df_filtered.isnull().sum().sum())  # Total missing values
#print(df_filtered.isnull().sum())        # Missing values per column

print(df_filtered['PriceCategory'].value_counts().sort_index())

"""### Step 2. Correlation Analysis + Step 3. Continue with Scaling/Standardization

### Splitting Predictors, Dropping Highly Correlated Columns, and Standardizing
"""

df_cleaned = df_filtered.copy()

# Re-create the PriceCategory column using SalePrice from the imputed dataframe
# We need to use the SalePrice from the imputed df_cleaned for binning
#df_cleaned['PriceCategory'] = pd.cut(df_cleaned['SalePrice'], bins=bins, labels=labels, include_lowest=True)

# Separate features (X) and target (y)
X = df_cleaned.drop(columns=["SalePrice", "PriceCategory", "Id"]) # Drop SalePrice, PriceCategory and Id
y = df_cleaned["PriceCategory"]

print("Shape BEFORE one-hot encoding and standardization:", X.shape)
print(y.value_counts().sort_index())

# Identify numerical and categorical columns for processing
numerical_cols = X.select_dtypes(include=np.number).columns
categorical_cols = X.select_dtypes(exclude=np.number).columns


# Apply one-hot encoding to categorical features
X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

print("Shape AFTER one-hot encoding:", X.shape)
print(y.value_counts().sort_index())

# Compute correlation matrix AFTER one-hot encoding
corr_matrix = X.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Identify columns to drop based on high correlation (threshold > 0.85)
to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]
print(y.value_counts().sort_index())

# Drop the highly correlated columns
X_reduced = X.drop(columns=to_drop)

print("Shape AFTER dropping highly correlated columns:", X_reduced.shape)

# Preview cleaned predictors before standardization
print("\nPreview of cleaned predictors (before standardization):")
print(X_reduced.head())
print(y.value_counts().sort_index())

# Verify missing values before standardization
print(X_reduced.isnull().sum().sum())  # Should be 0
print(y.value_counts().sort_index())

# Standardize the numerical columns
# Identify numerical columns that are still present in X_reduced
numerical_cols_in_reduced = X_reduced.select_dtypes(include=np.number).columns

# Use a ColumnTransformer to apply StandardScaler only to these numerical columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols_in_reduced)
    ],
    remainder='passthrough' # Keep other columns (one-hot encoded) as they are
)

X_scaled = preprocessor.fit_transform(X_reduced)
print(y.value_counts().sort_index())

# original one-hot encoded column names from X_reduced that were not numerical.
original_numerical_cols_in_reduced = [col for col in numerical_cols if col in X_reduced.columns]
original_other_cols_in_reduced = [col for col in X_reduced.columns if col not in numerical_cols]
X_scaled_df = pd.DataFrame(X_scaled, columns=list(numerical_cols_in_reduced) + original_other_cols_in_reduced)


# Confirm means and stds for numerical columns after standardization
print("\nMeans after standardization (should be ~0) for numerical columns:")
print(X_scaled_df[numerical_cols_in_reduced].mean().head())

print("\nStds after standardization (should be ~1) for numerical columns:")
print(X_scaled_df[numerical_cols_in_reduced].std().head())

print("\nFinal cleaned and standardized predictors shape:", X_scaled_df.shape)
print(y.value_counts().sort_index())

"""### Summary of Results:

- Shape Before: (1399 rows, 189 columns)
- Initial dataset after one-hot encoding and outlier removal.

- Computed Correlation Matrix:
 -  Measured pairwise relationships between predictors to detect multicollinearity.

- Columns Dropped Due to High Correlation: 13
 - Removed predictors with >0.85 correlation to avoid redundancy and improve model interpretability.

- Shape After Dropping Columns: (1399 rows, 176 columns)
 - Confirmed correct reduction in features.

- Standardized All Predictors:
 - Scaled features to mean â‰ˆ0 and standard deviation â‰ˆ1 to ensure comparability and support model convergence.

- Means After Scaling: ~0
 - Verified successful centering of each feature.

- Standard Deviations After Scaling: ~1
 - Verified correct scaling to unit variance.

### Outcome:
 - This cleaned and standardized dataset (X_scaled_df) is ready for regression modeling (Lasso, SVM, etc.) with reduced multicollinearity, balanced scales, and clear feature structure.

## Step 5. Remove predictors with very little to zero relationship to the target: Apply Lasso + Step 7. Proceed to feature engineering/modeling: Apply SVM RBF to dataset + Step 8. Model Evaluation

### Feature Selection Using:
 - Lasso
 - RFE

#### Lasso (Initial Screening)
"""

#Step 1: Verify X_scaled_df and y
print(X_scaled_df.shape, y.shape)

print("Class distribution in y:")
print(y.value_counts())

# Import necessary libraries
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel, RFE
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import numpy as np

# Step 1: Verify X_scaled_df and y
print(X_scaled_df.shape, y.shape)

# Check for NaNs in X_scaled_df before fitting Lasso
print("Checking for NaNs in X_scaled_df before Lasso:")
print(X_scaled_df.isnull().sum().sum())
if X_scaled_df.isnull().sum().sum() > 0:
    print("NaN values found in X_scaled_df. Investigating the source of NaNs.")

# Remove rows with NaN in the target variable y and reset index for alignment
nan_in_y = y.isnull()
if nan_in_y.sum() > 0:
    print(f"Removing {nan_in_y.sum()} rows with NaN values in the target variable y.")
    # Reset index of y to align with X_scaled_df's integer index
    y_reset_index = y.reset_index(drop=True)
    # Filter both X_scaled_df and y using the boolean index from the reset y
    X_scaled_df = X_scaled_df[~nan_in_y.reset_index(drop=True)]
    y = y_reset_index[~nan_in_y.reset_index(drop=True)]
    print(f"Shapes after removing NaNs in y: X_scaled_df: {X_scaled_df.shape}, y: {y.shape}")


# Lasso Feature Selection (Logistic Regression with L1 regularization)
# Using the cleaned and standardized data X_scaled_df
lasso = LogisticRegression(penalty='l1', solver='saga', C=0.1, max_iter=10000, random_state=42) # Increased max_iter
lasso.fit(X_scaled_df, y)

lasso_model = SelectFromModel(lasso, prefit=True)
lasso_features = X_scaled_df.columns[lasso_model.get_support()]

"""#### Visualization: Coeficients from Lasso"""

import numpy as np
import matplotlib.pyplot as plt

# Visualize Lasso coefficients
coef_abs = np.abs(lasso.coef_).max(axis=0)
lasso_coef_df = pd.Series(coef_abs, index=X_scaled_df.columns).sort_values(ascending=False).head(20)

plt.figure(figsize=(10, 6))
lasso_coef_df.plot(kind='bar')
plt.title('Top 20 Features Selected by Lasso')
plt.ylabel('Coefficient Magnitude')
plt.tight_layout()
plt.show()

"""#### RFE (Further Refinement)"""

# RFE Feature Selection
logreg = LogisticRegression(max_iter=10000, random_state=42) # Increased max_iter
rfe = RFE(estimator=logreg, n_features_to_select=20)
rfe.fit(X_scaled_df, y)

rfe_features = X_scaled_df.columns[rfe.get_support()]

"""#### Visualization: RFE Selected Features"""

# Visualize RFE rankings
rfe_ranking_df = pd.Series(rfe.ranking_, index=X_scaled_df.columns).sort_values().head(20)

plt.figure(figsize=(10, 6))
rfe_ranking_df.plot(kind='bar')
plt.title('Top 20 Features by RFE Ranking (lower is better)')
plt.ylabel('Ranking')
plt.tight_layout()
plt.show()

"""#### Further Refinement using Random Forest

#### Random Forest Feature Selection
"""

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_scaled_df, y)

rf_importances = pd.Series(rf.feature_importances_, index=X_scaled_df.columns).sort_values(ascending=False).head(20)

"""#### Visualization: Feature importance using Random Forest"""

plt.figure(figsize=(10, 6))
rf_importances.plot(kind='bar')
plt.title('Top 20 Feature Importances from Random Forest')
plt.ylabel('Importance')
plt.tight_layout()
plt.show()

"""#### Combine feature selection results"""

# Combine features from Lasso, RFE, and Random Forest
combined_features = pd.concat([pd.Series(lasso_features), pd.Series(rfe_features), pd.Series(rf_importances.index)]).value_counts()

# Select features identified by at least two methods
final_features = combined_features[combined_features >= 2].index.tolist()

print(f'Total combined features selected: {len(final_features)}')
print('Final selected features:', final_features)

import plotly.express as px
import pandas as pd

# Convert the Series to DataFrame for Plotly
combined_features_df = combined_features.reset_index()
combined_features_df.columns = ['Feature', 'Frequency']
combined_features_df = combined_features_df.sort_values(by='Frequency', ascending=True)

"""#### Visualization: Feature Selection with Combined Results"""

# Plot using Plotly
fig = px.bar(
    combined_features_df,
    x='Frequency',
    y='Feature',
    orientation='h',
    title='Frequency of Features Selected by Lasso, RFE, and Random Forest',
    height=1600,  # Increase height for vertical scroll space
    width=900
)

fig.update_layout(
    yaxis=dict(
        tickfont=dict(size=10),
    ),
    xaxis=dict(title='Selection Frequency'),
    margin=dict(l=200, r=20, t=50, b=20)
)

fig.show()

"""#### Refine DataFrame with Feature Selection"""

# Step 3: Refine dataframe to include only selected features for SVM RBF
X_final = X_scaled_df[final_features]

"""## Train Model Using SVM and RBF:"""

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# Step 7 Applying SVM RBF
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42, stratify=y)

svm_rbf = SVC(kernel='rbf', random_state=42)
svm_rbf.fit(X_train, y_train)

print("Step 7 complete: SVM with RBF kernel has been trained.")

y_pred = svm_rbf.predict(X_test)
print("Predictions on test set:", y_pred[:10])  # Shows first 10 predictions

print("Classes recognized by the model:", svm_rbf.classes_)

from sklearn.metrics import classification_report, confusion_matrix

print("\nClassification Report:\n")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:\n")
print(confusion_matrix(y_test, y_pred))

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# Step 8 Model Evaluation
# 4. HYPERPARAMETER TUNING with CROSS-VALIDATION
# Use a Pipeline to include StandardScaler within the GridSearchCV
pipeline = Pipeline([
    ('scaler', StandardScaler()), # Although data is already scaled, keeping this in pipeline for robustness
    ('classifier', SVC(kernel='rbf', random_state=42))
])

param_grid = {
    'classifier__C': [0.01, 0.1, 1, 10, 100],
    'classifier__gamma': ['scale', 0.1, 0.01, 0.001] # Fixed the missing quote here
}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy', verbose=2)
grid_search.fit(X_train, y_train)

print("Best Hyperparameters:", grid_search.best_params_)
print("Best Cross-validation Accuracy:", grid_search.best_score_)

# 5. FINAL TEST SET EVALUATION (this is your last, unbiased check)
final_model = grid_search.best_estimator_

y_pred = final_model.predict(X_test)

print("\nTest Set Classification Report:\n", classification_report(y_test, y_pred))
print("\nTest Set Confusion Matrix:\n", confusion_matrix(y_test, y_pred))